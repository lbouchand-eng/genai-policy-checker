{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695560ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import pdfplumber\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19393f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b98b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rasso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa162c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing AI_ACT.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 1/9 [00:51<06:50, 51.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing AudiovisualServices_SMA.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 2/9 [01:02<03:13, 27.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing Autor_rights_PI.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [01:14<02:03, 20.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing cnil_guide_personalSecurity.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [01:24<01:22, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing Digital_Markets_Act.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [01:46<01:13, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing Digital_Services_Act.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [02:19<01:10, 23.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing GDPR.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [02:47<00:49, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing Intellectual_Property.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [03:29<00:30, 30.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing NIS2_Cybersecu.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:53<00:00, 25.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Total chunks created: 6189\n",
      "ðŸ’¾ Chunks saved to eu_laws_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ---------- CONFIG ----------\n",
    "PDF_DIR = \"regulations_texts\"           # folder with your PDFs\n",
    "OUTPUT_JSONL = \"eu_laws_chunks.jsonl\"\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "    # Basic cleanup\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_eu_law(text, doc_title):\n",
    "    \"\"\"Chunk EU regulations/directives by recitals and articles.\"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # --- 1ï¸âƒ£ Split recitals ---\n",
    "    recitals = re.findall(r\"\\(\\d+\\).*?(?=\\(\\d+\\)|HAVE ADOPTED|TITLE I|Article\\s+1)\", text, re.S)\n",
    "    for r in recitals:\n",
    "        num = re.match(r\"\\((\\d+)\\)\", r)\n",
    "        if num:\n",
    "            chunks.append({\n",
    "                \"section\": \"Recital\",\n",
    "                \"number\": int(num.group(1)),\n",
    "                \"text\": r.strip(),\n",
    "                \"document_title\": doc_title\n",
    "            })\n",
    "\n",
    "    # --- 2ï¸âƒ£ Split articles ---\n",
    "    # find all \"Article X\" sections\n",
    "    article_blocks = re.split(r\"(?=Article\\s+\\d+)\", text)\n",
    "    for art in article_blocks:\n",
    "        match = re.match(r\"Article\\s+(\\d+)\", art)\n",
    "        if match:\n",
    "            art_num = int(match.group(1))\n",
    "            chunks.append({\n",
    "                \"section\": \"Article\",\n",
    "                \"article_number\": art_num,\n",
    "                \"text\": art.strip(),\n",
    "                \"document_title\": doc_title\n",
    "            })\n",
    "\n",
    "    # --- 3ï¸âƒ£ Fallback chunking for very long texts ---\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "    final_chunks = []\n",
    "    for c in chunks:\n",
    "        smalls = splitter.split_text(c[\"text\"])\n",
    "        for i, s in enumerate(smalls):\n",
    "            meta = c.copy()\n",
    "            meta[\"chunk_id\"] = f\"{c.get('section')}_{c.get('number', c.get('article_number', 'x'))}_{i}\"\n",
    "            meta[\"text\"] = s\n",
    "            final_chunks.append(meta)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def process_all_pdfs(pdf_dir):\n",
    "    \"\"\"Process all PDFs in a folder and return chunks.\"\"\"\n",
    "    all_chunks = []\n",
    "    for filename in tqdm(os.listdir(pdf_dir)):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            path = os.path.join(pdf_dir, filename)\n",
    "            print(f\"ðŸ“„ Processing {filename}...\")\n",
    "            text = extract_text_from_pdf(path)\n",
    "            chunks = chunk_eu_law(text, filename.replace(\".pdf\", \"\"))\n",
    "            all_chunks.extend(chunks)\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# ---------- Run the pipeline ----------\n",
    "all_chunks = process_all_pdfs(PDF_DIR)\n",
    "\n",
    "print(f\"\\nâœ… Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Save as JSONL\n",
    "with open(OUTPUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in all_chunks:\n",
    "        json.dump(c, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"ðŸ’¾ Chunks saved to {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d643e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
